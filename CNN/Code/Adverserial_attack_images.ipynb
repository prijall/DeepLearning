{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdbkqENDKgS1y/9M6qf4JR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn0blETSGrzR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "6789f09b-9d7f-4483-9abe-2873ee7b80b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch image. Status code: 404\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'original_image' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2ebe04cfbbc1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Apply the transformations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add batch dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Forward pass through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'original_image' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Load the pre-trained ResNet50 model\n",
        "model = resnet50(pretrained=True)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False  # Freeze the parameters\n",
        "\n",
        "# Image URL\n",
        "url = 'https://lionsvalley.co.za/wp-content/uploads/2015/11/africanelephant-square.jpg'\n",
        "\n",
        "# Fetch the image\n",
        "response = requests.get(url, stream=True)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    # Check if the content is an image\n",
        "    if \"image\" in response.headers[\"Content-Type\"]:\n",
        "        image_data = BytesIO(response.content)  # Convert the response to a file-like object\n",
        "        try:\n",
        "            original_image = Image.open(image_data).convert('RGB')  # Open image using PIL\n",
        "            original_image.show()  # Optionally display the image\n",
        "        except UnidentifiedImageError:\n",
        "            print(\"Error: Unable to identify the image file.\")\n",
        "            exit()\n",
        "    else:\n",
        "        print(f\"Error: The URL does not point to an image. Content-Type: {response.headers['Content-Type']}\")\n",
        "        exit()\n",
        "else:\n",
        "    print(f\"Failed to fetch image. Status code: {response.status_code}\")\n",
        "    exit()\n",
        "\n",
        "# Define image transformations (resize, tensor conversion, normalization)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # ResNet50 expects 224x224 images\n",
        "    transforms.ToTensor(),  # Convert to tensor and scale to [0,1]\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize based on ImageNet stats\n",
        "])\n",
        "\n",
        "# Apply the transformations\n",
        "image_tensor = transform(original_image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Forward pass through the model\n",
        "with torch.no_grad():\n",
        "    output = model(image_tensor)\n",
        "\n",
        "print(\"Model output shape:\", output.shape)  # Expected output: (1, 1000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_net_classes= ''\n",
        "image_net_ids=eval(image_net_classes)\n",
        "image_net_classes={i:j for j, i in image_net_ids.item()}"
      ],
      "metadata": {
        "id": "BJIpzC5tIA3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ function to normalize(img2tensor) and denormalize(tensor2img):\n",
        "from torchvision import transforms as T\n",
        "from torch.nn import functional as f\n",
        "\n",
        "normalize=T.Normalize([0.485, 0.456, 0.406])\n",
        "\n",
        "denormalize=T.Normalize(\n",
        "    [-0.485/0.229, -0.456/0.224, -0.406/0.225], #reversing mean normalization\n",
        "    [1/0.229, 1/0.224, 1/0.225] #reversing std\n",
        ")\n",
        "\n",
        "def Image2tensor(input):\n",
        "  x=normalize(input.clone().permute(2, 0, 1)/255.)[None]\n",
        "  return x\n",
        "\n",
        "def tensor2Image(input):\n",
        "  x=(denormalize(input[0].clone()).permute(1, 2, 0)*255.).type(torch.uint8)\n",
        "  return x"
      ],
      "metadata": {
        "id": "nD8_nxqszlRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_on_target(input):\n",
        "  model.eval()\n",
        "  show(input)\n",
        "  input=Image2tensor(input)\n",
        "  pred=model(input)\n",
        "  pred=F.softmax(pred, dim=-1)[0]\n",
        "  prob, clss=torch.max(pred, 0)\n",
        "  clss=image_net_ids[clss.item()]\n",
        "  print(f'PREDICTION: {clss} @ {prob.item()}')"
      ],
      "metadata": {
        "id": "j5IhRNjV2LiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import trange\n",
        "\n",
        "losses=[]\n",
        "\n",
        "def attack(image, model, target, epsilon=1e-6):\n",
        "  input=Image2tensor(image)\n",
        "  input.requires_grad=True\n",
        "  pred=model(input)\n",
        "  loss=nn.CrossEntropyLoss()(pred, target)\n",
        "  loss.backward()\n",
        "  losses.append(loss.mean().item())\n",
        "  output= input - epsilon * input.grad.sign()\n",
        "  output=tensor2Image(output)\n",
        "  del input\n",
        "  return output.detach()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0-joRTkCLgYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modified_images=[]\n",
        "desired_targets=['lemon', 'comic book', 'sax, saxophone']\n",
        "\n",
        "for target in desired_targets:\n",
        "  target=torch.tensor([image_net_classes[target]])\n",
        "  image_to_attack=original_image.clone()\n",
        "\n",
        "  for _ in trange(10):\n",
        "    image_to_attack=attack(image_to_attack, model, target)\n",
        "  modified_images.append(image_to_attack)\n",
        "\n",
        "for image in [original_image, *modified_images]:\n",
        "  predict_on_image(image)\n",
        "  inspect(image)"
      ],
      "metadata": {
        "id": "YjS7R5ASM0go"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}