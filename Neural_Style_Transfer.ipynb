{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKS+Ut3ZE31IC5sI35Hu54"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms as T\n",
        "preprocess = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]),\n",
        "    T.Lambda(lambda x: x.mul_(255))\n",
        "])\n",
        "\n",
        "postprocess=T.Compose([\n",
        "    T.Lambda(lambda x: x.mul_(1./255)),\n",
        "    T.Normalize(\n",
        "        mean=[-0.485/0.299, -0.456/0.224, -0.406/0.225],\n",
        "        std=[1/0.229, 1/0.224, 1/0.255])\n",
        "])"
      ],
      "metadata": {
        "id": "eTp2rMEF3Oh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13egsIa92Wwd"
      },
      "outputs": [],
      "source": [
        "#@ Gram Matrix module:\n",
        "class GramMatrix(nn.Module):\n",
        "  def forward(self, input):\n",
        "    b, c, h, w=input.size()\n",
        "    feature=input.view(b, c, h*w)\n",
        "    G= feature @ feature.transpose(1, 2)\n",
        "    G.div_(h*w)\n",
        "    return G"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GramMSELoss(nn.Module):\n",
        "  def forward(self, input, target):\n",
        "    output=F.mse_loss(GramMatrix()(input), target)\n",
        "    return(out)"
      ],
      "metadata": {
        "id": "Sag7YbEm4RrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ VGG-19 modified:\n",
        "class vgg19_modified(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    #extracting the feature:\n",
        "    features=list(vgg19(pretrained=True).features)\n",
        "    self.features=nn.ModuleList(features).eval()\n",
        "\n",
        "  def forward(self, x, layers=[]):\n",
        "    order=np.argsort(layers)\n",
        "    _results, results=[], []\n",
        "    for ix, model in enumerate(self.features):\n",
        "      x=model(x)\n",
        "      if ix in layers: _ results.append(x)\n",
        "    for o in orderL results.append(_results[o])\n",
        "    return results if layers is not [] else x"
      ],
      "metadata": {
        "id": "TG_Viz-BEFbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg=vgg19_modified().todevice()"
      ],
      "metadata": {
        "id": "8q_FmT0QGJqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Importing content and style image:\n",
        "!wget https://www.dropbox.com/s/z1y0fy2r6z6m6py/60.jpg\n",
        "!wget https://www.dropbox.com/s/1svdliljyo0a98v/style_image.png"
      ],
      "metadata": {
        "id": "WCIlS2M-GSFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs=[Image.open(path).resize((512, 512)).convert('RGB') for path in ['style_image.png', '60.jpg']]\n",
        "style_image, content_images=[preprocess(img).to(device)[None] for img in imgs]"
      ],
      "metadata": {
        "id": "wMTHEdVeGcFb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}