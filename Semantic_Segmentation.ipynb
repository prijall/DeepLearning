{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPlmKhO+qXg4z/rvHpTpFM/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@ Downloading necessary data:\n",
        "import os\n",
        "if not os.path.exists('dataset1'):\n",
        "  !wget -q https://www.dropbox.com/s/0pigmmmynbf9xwq/dataset1.zip\n",
        "  !unzip -q dataset1.zip\n",
        "  !rm dataset1.zip\n",
        "  !pip install -q torch_snippets pytorch_model_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qE9HCC-bxll",
        "outputId": "4352b66c-e5c5-463d-e211-16bf4bef5752"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.7/218.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Importing necessaries dependencies:\n",
        "import torch\n",
        "from torch_snippets import *\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn\n",
        "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2"
      ],
      "metadata": {
        "id": "98j7VB54chxM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ function for image transformation:\n",
        "tfms=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225]) # accord to imagenet dataset\n",
        "])"
      ],
      "metadata": {
        "id": "4lMQHX20dJ9d"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Fetching input and output images for training:\n",
        "class SegData(Dataset):\n",
        "  def __init__(self, split):\n",
        "    self.items=stems(f'dataset1/images_prepped_{split}')\n",
        "    self.split=split\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.items)\n",
        "\n",
        "  def __getitem__(self, ix):\n",
        "    image=read(f'dataset1/images_prepped_{self.split}/ {self.items[ix]}.png', 1)\n",
        "    image=cv2.resize(image, (224, 224))\n",
        "    mask=read(f'dataset1/images_prepped_{self.split}/ {self.items[ix]}.png')[:, :, 0]\n",
        "    mask=cv2.resize(mask, (224, 224))\n",
        "    return image, mask\n",
        "\n",
        "  # random image index for debugging purpose:\n",
        "  def choose(self):\n",
        "    return self[randint(len(self))]\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "    ims, masks=list(zip(*batch))\n",
        "    ims=torch.cat([tfms(im.copy()/255.)[None] for im in ims]).float().to(device)\n",
        "    ce_mask=torch.cat([torch.Tensor(mask[None]) for mask in masks]).long().to(device)\n",
        "    return ims, ce_mask"
      ],
      "metadata": {
        "id": "AqHpb_L0fMYS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ training and valid dataset:\n",
        "train_ds=SegData('train')\n",
        "valid_ds=SegData('test')\n",
        "trn_dl=DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=train_ds.collate_fn)\n",
        "val_dl=DataLoader(valid_ds, batch_size=1, shuffle=True, collate_fn=valid_ds.collate_fn)\n"
      ],
      "metadata": {
        "id": "-m_iuCF3o6dL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Architecture for image segmentation"
      ],
      "metadata": {
        "id": "_IsCunlsd3tI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ defining convolution blocks:\n",
        "def conv(in_channels, out_channels):\n",
        "  return nn.Sequential(\n",
        "      nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(inplace=True)\n",
        "  )"
      ],
      "metadata": {
        "id": "tF0dt_5id81c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ defining Up-Convolution:\n",
        "def up_conv(in_channels, out_channels):\n",
        "  return nn.Sequential(\n",
        "      nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2), # ensure image upscaling\n",
        "      nn.ReLU(inplace=True)\n",
        "  )"
      ],
      "metadata": {
        "id": "9utytDAkfUfP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Defining Network Class:\n",
        "from torchvision.models import vgg16_bn # for large scale\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, pretrained=True, out_channels=12):\n",
        "    super().__init__()\n",
        "    self.encoder=vgg16_bn(pretrained=pretrained).features # excluding FC at end\n",
        "\n",
        "    # encoder blocks\n",
        "    self.block1=nn.Sequential(*self.encoder[:6])\n",
        "    self.block2=nn.Sequential(*self.encoder[6:13])\n",
        "    self.block3=nn.Sequential(*self.encoder[13:20])\n",
        "    self.block4=nn.Sequential(*self.encoder[20:27])\n",
        "    self.block5=nn.Sequential(*self.encoder[27:34])\n",
        "\n",
        "    self.bottleneck=nn.Sequential(*self.encoder[34:]) #acts between encoder and decoder\n",
        "    self.conv_bottleneck=conv(512, 1024)\n",
        "\n",
        "    self.up_conv6=up_conv(1024, 512)\n",
        "    self.conv6=conv(512 + 512, 512)\n",
        "    self.up_conv7=up_conv(512, 256)\n",
        "    self.conv7=conv(512 + 256, 256)\n",
        "    self.up_conv8=up_conv(256, 128)\n",
        "    self.conv8=conv(128 + 256, 128)\n",
        "    self.up_conv9=up_conv(128, 64)\n",
        "    self.conv9=conv(128 + 64, 64)\n",
        "    self.up_conv10=up_conv(64, 32)\n",
        "    self.conv10=conv(32 + 64, 32)\n",
        "\n",
        "    self.conv11=nn.Conv2d(32, out_channels, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    block1=self.block1(x)\n",
        "    block2=self.block2(block1)\n",
        "    block3=self.block2(block2)\n",
        "    block4=self.block2(block3)\n",
        "    block5=self.block2(block4)\n",
        "\n",
        "    bottleneck=self.bottleneck(block5)\n",
        "    x=self.conv_bottleneck(bottleneck)\n",
        "\n",
        "    x=self.up_conv6(x)\n",
        "    x=torch.cat([x, block5], dim=1)\n",
        "    x=self.conv6(x)\n",
        "\n",
        "    x=self.up_conv7(x)\n",
        "    x=torch.cat([x, block4], dim=1)\n",
        "    x=self.conv7(x)\n",
        "\n",
        "    x=self.up_conv8(x)\n",
        "    x=torch.cat([x, block3], dim=1)\n",
        "    x=self.conv8(x)\n",
        "\n",
        "    x=self.up_conv9(x)\n",
        "    x=torch.cat([x, block2], dim=1)\n",
        "    x=self.conv9(x)\n",
        "\n",
        "\n",
        "    x=self.up_conv10(x)\n",
        "    x=torch.cat([x, block1], dim=1)\n",
        "    x=self.conv10(x)\n",
        "\n",
        "    x=self.conv11(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Aafjc45IgLMk"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}