{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKFswztzHcUZ/9MgPzv5wO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MnFZEmaqZvmY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np, cv2, io, os, re, string, time, datetime\n",
        "import seaborn as sns, sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import (TextVectorization)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Downloading datasets:\n",
        "!wget https://www.manythings.org/anki/fra-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9Bkvwggaepq",
        "outputId": "4d4eca0d-a75f-405b-b92a-c540cc9a4be5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-08 15:55:09--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7943074 (7.6M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.57M  4.38MB/s    in 1.7s    \n",
            "\n",
            "2025-01-08 15:55:12 (4.38 MB/s) - ‘fra-eng.zip’ saved [7943074/7943074]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/fra-eng.zip' -d '/content/dataset' # -d flag specifies directories"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzuI4hW6aq_q",
        "outputId": "393be319-b415-4f5d-dfbb-24e01da2893a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/fra-eng.zip\n",
            "  inflating: /content/dataset/_about.txt  \n",
            "  inflating: /content/dataset/fra.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Preprocessing"
      ],
      "metadata": {
        "id": "cPc5HmT4bR4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_dataset=tf.data.TextLineDataset('/content/dataset/fra.txt') #each line is treated as separate string"
      ],
      "metadata": {
        "id": "0GEjkt5-bT6B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in text_dataset.take(3):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y__LTpfEbuhy",
        "outputId": "8504fb67-c33e-4c7c-99e0-50932c7e32f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Go.\\tMarche.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Go.\\tEn route !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Setting up the Parameters:\n",
        "VOCAB_SIZE=20000 #unique tokens from dataset, setting value 20000 for efficiency\n",
        "ENGLISH_SEQUENCE_LENGTH=32 #max length of i/p sequence[in tokens]\n",
        "FRENCH_SEQUENCE_LENGTH=32 #max len of o/p sequence[in tokens]\n",
        "EMBEDDINGS_DIM=512 #size of vectors to represent tokens(as per paper)\n",
        "BATCH_SIZE=128 #for data size processed during training"
      ],
      "metadata": {
        "id": "yHGUPjuwb6Gx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ for english word:\n",
        "english_vectorize_layer=TextVectorization(\n",
        "                      standardize='lower_and_strip_punctuation',\n",
        "                      max_tokens=VOCAB_SIZE,\n",
        "                      output_mode='int', #mapping wrt to the integer index\n",
        "                      output_sequence_length=ENGLISH_SEQUENCE_LENGTH\n",
        ")\n",
        "\n",
        "#@ for french word:\n",
        "french_vectorize_layer=TextVectorization(\n",
        "                       standardize='lower_and_strip_punctuation',\n",
        "                       max_tokens=VOCAB_SIZE,\n",
        "                       output_mode='int',\n",
        "                       output_sequence_length=FRENCH_SEQUENCE_LENGTH\n",
        ")"
      ],
      "metadata": {
        "id": "hAuTBC_EeFMK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seperator(input_text):\n",
        "  split_text=tf.strings.split(input_text, '\\t')\n",
        "  return {\n",
        "      'input_1':split_text[0:1],\n",
        "      'input_2':'starttoken' + split_text[1:2]\n",
        "      }, split_text[1:2]+' endtoken'"
      ],
      "metadata": {
        "id": "nlKHREVNhT2i"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text='hello\\tprijal'\n",
        "seperator(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndmYklVTi-IJ",
        "outputId": "e6933387-261d-4c80-97a9-4ecdf03b37b4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'hello'], dtype=object)>,\n",
              "  'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttokenprijal'], dtype=object)>},\n",
              " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'prijal endtoken'], dtype=object)>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Initializing dataset:\n",
        "init_dataset=text_dataset.map(seperator)"
      ],
      "metadata": {
        "id": "ustOzRdroLei"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in init_dataset.take(3):\n",
        "  print(i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukk0KU5TpH3a",
        "outputId": "9dbc9d44-73f1-4327-cea1-4268732d2389"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttokenVa !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Va ! endtoken'], dtype=object)>)\n",
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttokenMarche.'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Marche. endtoken'], dtype=object)>)\n",
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttokenEn route !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'En route ! endtoken'], dtype=object)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocab Creation"
      ],
      "metadata": {
        "id": "JCOUUsunpsix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_training_data=init_dataset.map(lambda x, y:x['input_1'])\n",
        "english_vectorize_layer.adapt(english_training_data)\n",
        "\n",
        "french_training_data=init_dataset.map(lambda x, y:y)\n",
        "french_vectorize_layer.adapt(french_training_data)\n"
      ],
      "metadata": {
        "id": "zDyJI27QpvNp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}