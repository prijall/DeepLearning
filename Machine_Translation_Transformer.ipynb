{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MnFZEmaqZvmY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf### models\n",
        "import numpy as np### math computations\n",
        "import matplotlib.pyplot as plt### plotting bar chart\n",
        "import sklearn### machine learning library\n",
        "import cv2## image processing\n",
        "from sklearn.metrics import confusion_matrix, roc_curve### metrics\n",
        "import seaborn as sns### visualizations\n",
        "import datetime\n",
        "import pathlib\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "from numpy import random\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "from keras.models import Model\n",
        "from keras.layers import Layer\n",
        "from keras.layers import (Dense,Flatten,SimpleRNN,InputLayer,Conv1D,Bidirectional,GRU,LSTM,BatchNormalization,Dropout,Input, Embedding,TextVectorization)\n",
        "from keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import MultiHeadAttention, LayerNormalization\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from tensorboard.plugins import projector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9Bkvwggaepq",
        "outputId": "4bc9e129-e956-4e02-fbfd-a7df5f6c5fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-21 16:01:16--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7943074 (7.6M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.57M  14.3MB/s    in 0.5s    \n",
            "\n",
            "2025-01-21 16:01:16 (14.3 MB/s) - ‘fra-eng.zip’ saved [7943074/7943074]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@ Downloading datasets:\n",
        "!wget https://www.manythings.org/anki/fra-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzuI4hW6aq_q",
        "outputId": "b44f95c5-8278-4439-e13c-2220d76e2226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/fra-eng.zip\n",
            "  inflating: /content/dataset/_about.txt  \n",
            "  inflating: /content/dataset/fra.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/fra-eng.zip' -d '/content/dataset' # -d flag specifies directories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPc5HmT4bR4C"
      },
      "source": [
        "#### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0GEjkt5-bT6B"
      },
      "outputs": [],
      "source": [
        "text_dataset=tf.data.TextLineDataset('/content/dataset/fra.txt') #each line is treated as separate string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y__LTpfEbuhy",
        "outputId": "efccba15-582c-495c-ac06-786160811666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Go.\\tMarche.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Go.\\tEn route !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for i in text_dataset.take(3):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yHGUPjuwb6Gx"
      },
      "outputs": [],
      "source": [
        "#@ Setting up the Parameters:\n",
        "VOCAB_SIZE=20000 #unique tokens from dataset, setting value 20000 for efficiency\n",
        "ENGLISH_SEQUENCE_LENGTH=32 #max length of i/p sequence[in tokens]\n",
        "FRENCH_SEQUENCE_LENGTH=32 #max len of o/p sequence[in tokens]\n",
        "EMBEDDINGS_DIM=512 #size of vectors to represent tokens(as per paper)\n",
        "BATCH_SIZE=128 #for data size processed during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hAuTBC_EeFMK"
      },
      "outputs": [],
      "source": [
        "#@ for english word:\n",
        "english_vectorize_layer=TextVectorization(\n",
        "                      standardize='lower_and_strip_punctuation',\n",
        "                      max_tokens=VOCAB_SIZE,\n",
        "                      output_mode='int', #mapping wrt to the integer index\n",
        "                      output_sequence_length=ENGLISH_SEQUENCE_LENGTH\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "french_vectorize_layer = TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=FRENCH_SEQUENCE_LENGTH\n",
        ")"
      ],
      "metadata": {
        "id": "TBiKsFzQJqIZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nlKHREVNhT2i"
      },
      "outputs": [],
      "source": [
        "def selector(input_text):\n",
        "  split_text = tf.strings.split(input_text,'\\t')\n",
        "  return {'input_1':split_text[0:1],'input_2':'starttoken '+split_text[1:2]},split_text[1:2]+' endtoken'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ustOzRdroLei"
      },
      "outputs": [],
      "source": [
        "#@ Initializing dataset:\n",
        "split_dataset = text_dataset.map(selector)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def separator(input_text):\n",
        "  split_text = tf.strings.split(input_text,'\\t')\n",
        "  return split_text[0:1],'starttoken '+split_text[1:2]+' endtoken'"
      ],
      "metadata": {
        "id": "VZZ_I2fhJ1GJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_dataset = text_dataset.map(separator)"
      ],
      "metadata": {
        "id": "ernmN6JeJ6Cx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukk0KU5TpH3a",
        "outputId": "978ccd79-9160-46a2-e4d6-87408b743fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Va ! endtoken'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Marche. endtoken'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken En route ! endtoken'], dtype=object)>)\n"
          ]
        }
      ],
      "source": [
        "for i in init_dataset.take(3):\n",
        "  print(i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCOUUsunpsix"
      },
      "source": [
        "### Vocab Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zDyJI27QpvNp"
      },
      "outputs": [],
      "source": [
        "english_training_data=init_dataset.map(lambda x,y:x) # input x,y and output x\n",
        "english_vectorize_layer.adapt(english_training_data) # adapt the vectorize_layer to the training data\n",
        "\n",
        "french_training_data=init_dataset.map(lambda x,y:y) # input x,y,z and output y\n",
        "french_vectorize_layer.adapt(french_training_data) # adapt the vectorize_layer to the training data\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "V5DDzmf8L_fZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def vectorizer(inputs,output):\n",
        "  return {'input_1':english_vectorize_layer(inputs['input_1']),\n",
        "          'input_2':french_vectorize_layer(inputs['input_2'])},french_vectorize_layer(output)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDlEbt5XM4r5",
        "outputId": "46253c50-3fad-4b15-c5fd-4c4058a9e323"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_MapDataset element_spec=({'input_1': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'input_2': TensorSpec(shape=(None,), dtype=tf.string, name=None)}, TensorSpec(shape=(None,), dtype=tf.string, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "split_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HW8hWSGHNAbZ"
      },
      "outputs": [],
      "source": [
        "dataset=split_dataset.map(vectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5R0qqzbNYFZ",
        "outputId": "46ed3944-17c3-4c95-860a-231184470099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Va ! endtoken'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Marche. endtoken'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken En route ! endtoken'], dtype=object)>)\n"
          ]
        }
      ],
      "source": [
        "for i in init_dataset.take(3):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Z-KNkkdXNfBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43899f90-df27-4106-e099-0a2d0f7f8f60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1, 32), dtype=int64, numpy=\n",
            "array([[45,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>, 'input_2': <tf.Tensor: shape=(1, 32), dtype=int64, numpy=\n",
            "array([[  2, 104,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0]])>}, <tf.Tensor: shape=(1, 32), dtype=int64, numpy=\n",
            "array([[104,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0]])>)\n"
          ]
        }
      ],
      "source": [
        "for i in dataset.take(1):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vQeOs1m8Nooa"
      },
      "outputs": [],
      "source": [
        "dataset=dataset.shuffle(2048).unbatch().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3MPd8xmaN45p"
      },
      "outputs": [],
      "source": [
        "NUM_BATCHES=int(200000/BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "T3LvhBFsOCcy"
      },
      "outputs": [],
      "source": [
        "#@ Training and testing split\n",
        "train_dataset=dataset.take(int(0.9*NUM_BATCHES))\n",
        "val_dataset=dataset.skip(int(0.9*NUM_BATCHES))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXIaPrbnOWh5"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UuLziKTkOZyh"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(model_size, SEQUENCE_LENGTH): # d_model\n",
        "  output = []\n",
        "  for pos in range(SEQUENCE_LENGTH):\n",
        "    PE = np.zeros((model_size)) # initilizing with zeros\n",
        "    for i in range(model_size):\n",
        "      if i % 2 == 0: # even positions, sin formula is used according to paper\n",
        "        PE[i] = np.sin(pos/(10000**(i/model_size)))\n",
        "      else: # odd positions, cos formula is used as mentioned in the paper\n",
        "        PE[i] = np.cos(pos/(10000**((i-1)/model_size)))\n",
        "    output.append(tf.expand_dims(PE, axis = 0))\n",
        "\n",
        "  out = tf.concat(output, axis=0)\n",
        "  out = tf.expand_dims(out, axis=0)\n",
        "  return tf.cast(out, dtype=tf.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqa_KTf3T5Is"
      },
      "source": [
        "### Input Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "xflRdAcHT89U"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Lambda\n",
        "class Embeddings(Layer):\n",
        "  def __init__(self, sequence_length, vocab_size, embedding_dim):\n",
        "    super(Embeddings, self).__init__()\n",
        "    self.token_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = positional_encoding(self.embedding_dim, self.sequence_length) # PE adding here\n",
        "    return embedded_tokens + embedded_positions # final output for inputs\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "     return Lambda(lambda x: tf.math.not_equal(x, 0))(inputs) # masking function for checking if there are pad tokens(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noFlpky0YjJX"
      },
      "source": [
        "### Custome Attention Layer\n",
        "\n",
        "- Self attention layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2DVoYgd-YrMN"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomSelfAttention(Layer):\n",
        "  def __init__(self, model_size):\n",
        "    super(CustomSelfAttention, self).__init__()\n",
        "    self.model_size = model_size\n",
        "\n",
        "  def call(self, query, key, value, masking):\n",
        "    #### Compute Scores ####\n",
        "    score = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    #### Scaling ####\n",
        "    score = score / tf.math.sqrt(tf.cast(self.model_size, dtype=tf.float32))\n",
        "\n",
        "    #### Masking ####\n",
        "    masking = tf.cast(masking, dtype=tf.float32)\n",
        "    score -= (1.0 - masking) * 1e10\n",
        "\n",
        "    #### Attention Weights ####\n",
        "    attention_weights = tf.nn.softmax(score, axis=-1) * masking\n",
        "\n",
        "    #### Weighted Sum ####\n",
        "    head_output = tf.matmul(attention_weights, value)\n",
        "\n",
        "    #### Output ####\n",
        "    return head_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-headed Attention\n",
        "- Multihead Attention allows model to focus on different part of input sequence simultaneously and combine these prespective into comprehensive representation.\n",
        "\n",
        "- For example: \"Harry saw a man with binoculars'. This sentence can have two meanings, they are either it can be harry saw a man using binoculars or it can be harry saw a man who has binoculars. These both can be correct. So transformer has to understand both these meanings which self-attention fails to recognize that's why multi-head attention is used."
      ],
      "metadata": {
        "id": "f1Vqrwn1Sdsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class CustomMultiHeadAttention(Layer):\n",
        "  def __init__(self, num_heads, key_dim):\n",
        "    super(CustomMultiHeadAttention, self).__init__()\n",
        "\n",
        "    self.num_heads = num_heads\n",
        "    self.dense_q = [Dense(key_dim//num_heads) for _ in range(num_heads)]\n",
        "    self.dense_k = [Dense(key_dim//num_heads) for _ in range(num_heads)]\n",
        "    self.dense_v = [Dense(key_dim//num_heads) for _ in range(num_heads)]\n",
        "    self.dense_o = Dense(key_dim)\n",
        "    self.attention = CustomSelfAttention(key_dim)\n",
        "\n",
        "  def call(self, query, key, value, attention_mask):\n",
        "    heads = []\n",
        "\n",
        "    for i in range(self.num_heads): # for each head\n",
        "      print(f'head-{i}', self.dense_q[i](query).shape)\n",
        "      head = self.self_attention(self.dense_q[i](query), self.dense_k[i](key),\n",
        "                                 self.dense_v[i](value), attention_mask)\n",
        "\n",
        "      heads.append(head)\n",
        "    heads = tf.concat(heads, axis=2) # concatenating all heads\n",
        "    heads = self.dense_o(heads) # passing all heads through a linear layer for the final output\n",
        "    return heads\n",
        "\n"
      ],
      "metadata": {
        "id": "XadG1w2LSioN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "PfIpdo0cZhm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TransformerEncoder(Layer):\n",
        "  def __init__(self, embedding_dims, dense_dims, num_heads):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    self.embedding_dims = embedding_dims\n",
        "    self.dense_dims = dense_dims\n",
        "    self.num_heads = num_heads\n",
        "    self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dims)\n",
        "\n",
        "    self.dense_proj = tf.keras.Sequential([\n",
        "        Dense(self.dense_dims, activation=\"relu\"),\n",
        "        Dense(self.embedding_dims),\n",
        "    ])\n",
        "    self.layernorm_1 = LayerNormalization()\n",
        "    self.layernorm_2 = LayerNormalization()\n",
        "    self.supports_masking = True\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    # print(mask)\n",
        "    if mask is not None:\n",
        "      mask = tf.cast(mask[:, tf.newaxis, :], dtype='int32')\n",
        "      # print(mask)\n",
        "      T = tf.shape(mask)[2]\n",
        "      padding_mask = tf.repeat(mask, T, axis=1)\n",
        "      # print(padding_mask)\n",
        "\n",
        "    attention_output = self.attention(query=inputs, value=inputs, key=inputs, attention_mask=padding_mask)\n",
        "\n",
        "    proj_input = self.layernorm_1(inputs + attention_output)\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "    return self.layernorm_2(proj_input + proj_output)\n",
        ""
      ],
      "metadata": {
        "id": "5PAqADAlZnyF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "eNcbK0YVmEDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(Layer):\n",
        "  def __init__(self, embedding_dims, latent_dims, num_heads):\n",
        "    super(TransformerDecoder, self).__init__()\n",
        "    self.embedding_dims = embedding_dims\n",
        "    self.latent_dims = latent_dims\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_1 = MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embedding_dims\n",
        "    ) # self attention\n",
        "    self.attention_2 = MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embedding_dims\n",
        "    ) # cross-attention with encoder's outputs\n",
        "    self.dense_proj = tf.keras.Sequential(\n",
        "        [Dense(latent_dims, activation='relu'), Dense(embedding_dims)]\n",
        "    ) # feed forward layer\n",
        "    self.layernorm_1 = LayerNormalization() # layer norm for all three layers as in paper\n",
        "    self.layernorm_2 = LayerNormalization()\n",
        "    self.layernorm_3 = LayerNormalization()\n",
        "    self.supports_masking = True # this is special because of decoder\n",
        "\n",
        "  def call (self, inputs, encoder_outputs, enc_mask, mask=None):\n",
        "\n",
        "    combined_mask=None\n",
        "    cross_attn_mask = None\n",
        "\n",
        "    if mask is not None:\n",
        "      causal_mask = tf.linalg.band_part(\n",
        "          tf.ones([tf.shape(inputs)[0],\n",
        "                   tf.shape(inputs)[1],\n",
        "                   tf.shape(inputs)[1]], dtype=tf.int32), -1, 0)\n",
        "      # the role of causal mask is to prevent peeking into the future tokens for the decoder to predict better\n",
        "      # the band_part method makes it really easier to do this\n",
        "\n",
        "      mask = tf.cast(\n",
        "          mask[:, tf.newaxis, :], dtype='int32'\n",
        "      )\n",
        "      enc_mask = tf.cast(\n",
        "          enc_mask[:, tf.newaxis, :], dtype='int32'\n",
        "      )\n",
        "\n",
        "      T = tf.shape(mask)[2] # T is the number of queries from the decoder\n",
        "      padding_mask = tf.repeat(mask, T, axis=1)\n",
        "      cross_attn_mask = tf.repeat(enc_mask, T, axis=1)\n",
        "      combined_mask = tf.minimum(padding_mask, causal_mask) # the full mask for the masked mutli-head-connection\n",
        "      # print(f'Padding_mask: {padding_mask}')\n",
        "      # print(f'Causal_mask: {causal_mask}')\n",
        "      # print(f'Combined_mask: {combined_mask}')\n",
        "      # print(f'Cross_attention_mask: {cross_attn_mask}')\n",
        "\n",
        "    if combined_mask is None:\n",
        "            combined_mask = tf.ones([tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[1]], dtype=tf.int32)\n",
        "\n",
        "    if cross_attn_mask is None:\n",
        "            T = tf.shape(inputs)[1]  # Get the sequence length from inputs\n",
        "            cross_attn_mask = tf.ones([tf.shape(inputs)[0], T, T], dtype=tf.int32)\n",
        "\n",
        "\n",
        "    attention_output_1 = self.attention_1(\n",
        "        query=inputs, key=inputs, value=inputs,\n",
        "        attention_mask=combined_mask # the first layer which is the self attention for decoder\n",
        "    )\n",
        "\n",
        "    out_1 = self.layernorm_1(inputs + attention_output_1) # the first output + inputs added to be the input\n",
        "    # for the cross_attention layer\n",
        "\n",
        "    attention_output_2, scores = self.attention_2(\n",
        "        query=out_1, key=encoder_outputs, value=encoder_outputs,\n",
        "        attention_mask=cross_attn_mask,# the mask from cross attention just like encoder\n",
        "        return_attention_scores=True # returning score to visualize\n",
        "    )\n",
        "\n",
        "    out_2 = self.layernorm_2(out_1 + attention_output_2) # output 2 after adding and normalizing to be passed\n",
        "    # to feed forward layer for the final outputs\n",
        "\n",
        "    proj_output = self.dense_proj(out_2)\n",
        "\n",
        "    return self.layernorm_3(out_2 + proj_output), scores # the last norm layer"
      ],
      "metadata": {
        "id": "cKJioxNfmGoq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full transformer Model"
      ],
      "metadata": {
        "id": "-ebLcx7XHYjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIMS = 512\n",
        "LATENT_DIMS = 2048\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 1\n",
        "NUM_EPOCHS = 10\n",
        "attention_scores = {}"
      ],
      "metadata": {
        "id": "pN6sDoGWHbvj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = Input(shape=(None,), dtype='int64', name='input_1')\n",
        "embeddings = Embeddings(ENGLISH_SEQUENCE_LENGTH, VOCAB_SIZE, EMBEDDING_DIMS)\n",
        "x = embeddings(encoder_inputs)\n",
        "enc_mask = embeddings.compute_mask(encoder_inputs)\n",
        "\n",
        "\n",
        "for _ in range(NUM_LAYERS): # there can be N number of layers as mentioned by paper\n",
        "  x = TransformerEncoder(EMBEDDING_DIMS, LATENT_DIMS, NUM_HEADS)(x)\n",
        "encoder_outputs = x\n",
        "\n",
        "decoder_inputs = Input(shape=(None,), dtype='int64', name='input_2')\n",
        "x = Embeddings(FRENCH_SEQUENCE_LENGTH, VOCAB_SIZE, EMBEDDING_DIMS)(decoder_inputs)\n",
        "\n",
        "for i in range(NUM_LAYERS):\n",
        "  x, scores = TransformerDecoder(EMBEDDING_DIMS, LATENT_DIMS, NUM_HEADS)(x, encoder_outputs, enc_mask)\n",
        "  attention_scores[f'decoder_layer{i+1}_block2'] = scores\n",
        "\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "decoder_outputs = Dense(VOCAB_SIZE, activation='softmax')(x)\n",
        "\n",
        "attention_score_model = tf.keras.Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    attention_scores, name='attention_score_model'\n",
        ")\n",
        "\n",
        "\n",
        "transformer = tf.keras.Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    decoder_outputs, name='transformer'\n",
        ")\n",
        "\n",
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "KeI40mXvIMMq",
        "outputId": "8bb133ad-06c8-41fc-a875-ec1e74b7979d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:1383: UserWarning: Layer 'transformer_encoder' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encountered: ''cannot access local variable 'padding_mask' where it is not associated with a value''\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'transformer_encoder', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"transformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embeddings_2 (\u001b[38;5;33mEmbeddings\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m10,240,000\u001b[0m │ input_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda (\u001b[38;5;33mLambda\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ input_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embeddings_3 (\u001b[38;5;33mEmbeddings\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m10,240,000\u001b[0m │ input_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m10,503,168\u001b[0m │ embeddings_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ input_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_decoder       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m),      │     \u001b[38;5;34m18,905,600\u001b[0m │ embeddings_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)]     │                │ transformer_encoder[\u001b[38;5;34m0\u001b[0m… │\n",
              "│                           │                        │                │ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ transformer_decoder[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m20000\u001b[0m)      │     \u001b[38;5;34m10,260,000\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embeddings_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embeddings</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,240,000</span> │ input_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embeddings_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embeddings</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,240,000</span> │ input_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,503,168</span> │ embeddings_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_decoder       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,905,600</span> │ embeddings_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)]     │                │ transformer_encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                           │                        │                │ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_decoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,260,000</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m60,148,768\u001b[0m (229.45 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">60,148,768</span> (229.45 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m60,148,768\u001b[0m (229.45 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">60,148,768</span> (229.45 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMcXsITCDRsM+Ex19mVbIsO"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}