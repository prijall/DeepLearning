{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MnFZEmaqZvmY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf### models\n",
        "import numpy as np### math computations\n",
        "import matplotlib.pyplot as plt### plotting bar chart\n",
        "import sklearn### machine learning library\n",
        "import cv2## image processing\n",
        "from sklearn.metrics import confusion_matrix, roc_curve### metrics\n",
        "import seaborn as sns### visualizations\n",
        "import datetime\n",
        "import pathlib\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "from numpy import random\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "from keras.models import Model\n",
        "from keras.layers import Layer\n",
        "from keras.layers import (Dense,Flatten,SimpleRNN,InputLayer,Conv1D,Bidirectional,GRU,LSTM,BatchNormalization,Dropout,Input, Embedding,TextVectorization)\n",
        "from keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import MultiHeadAttention, LayerNormalization\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from tensorboard.plugins import projector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9Bkvwggaepq",
        "outputId": "2ec02077-5b59-4734-c926-7ee536ce8b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-22 15:47:18--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7943074 (7.6M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.57M  3.94MB/s    in 1.9s    \n",
            "\n",
            "2025-01-22 15:47:21 (3.94 MB/s) - ‘fra-eng.zip’ saved [7943074/7943074]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@ Downloading datasets:\n",
        "!wget https://www.manythings.org/anki/fra-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzuI4hW6aq_q",
        "outputId": "ac525eda-da49-4cfa-87dc-572e71c55283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/fra-eng.zip\n",
            "  inflating: /content/dataset/_about.txt  \n",
            "  inflating: /content/dataset/fra.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/fra-eng.zip' -d '/content/dataset' # -d flag specifies directories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPc5HmT4bR4C"
      },
      "source": [
        "#### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0GEjkt5-bT6B"
      },
      "outputs": [],
      "source": [
        "text_dataset=tf.data.TextLineDataset('/content/dataset/fra.txt') #each line is treated as separate string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y__LTpfEbuhy",
        "outputId": "cd886a25-7c90-402a-97f2-d890935f56b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Go.\\tMarche.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Go.\\tEn route !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for i in text_dataset.take(3):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yHGUPjuwb6Gx"
      },
      "outputs": [],
      "source": [
        "#@ Setting up the Parameters:\n",
        "VOCAB_SIZE=20000 #unique tokens from dataset, setting value 20000 for efficiency\n",
        "ENGLISH_SEQUENCE_LENGTH=32 #max length of i/p sequence[in tokens]\n",
        "FRENCH_SEQUENCE_LENGTH=32 #max len of o/p sequence[in tokens]\n",
        "EMBEDDINGS_DIM=512 #size of vectors to represent tokens(as per paper)\n",
        "BATCH_SIZE=128 #for data size processed during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hAuTBC_EeFMK"
      },
      "outputs": [],
      "source": [
        "#@ for english word:\n",
        "english_vectorize_layer=TextVectorization(\n",
        "                      standardize='lower_and_strip_punctuation',\n",
        "                      max_tokens=VOCAB_SIZE,\n",
        "                      output_mode='int', #mapping wrt to the integer index\n",
        "                      output_sequence_length=ENGLISH_SEQUENCE_LENGTH\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "french_vectorize_layer = TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=FRENCH_SEQUENCE_LENGTH\n",
        ")"
      ],
      "metadata": {
        "id": "TBiKsFzQJqIZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nlKHREVNhT2i"
      },
      "outputs": [],
      "source": [
        "def selector(input_text):\n",
        "  split_text = tf.strings.split(input_text,'\\t')\n",
        "  return {'input_1':split_text[0:1],'input_2':'starttoken '+split_text[1:2]},split_text[1:2]+' endtoken'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ustOzRdroLei"
      },
      "outputs": [],
      "source": [
        "#@ Initializing dataset:\n",
        "split_dataset = text_dataset.map(selector)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def separator(input_text):\n",
        "  split_text = tf.strings.split(input_text,'\\t')\n",
        "  return split_text[0:1],'starttoken '+split_text[1:2]+' endtoken'"
      ],
      "metadata": {
        "id": "VZZ_I2fhJ1GJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_dataset = text_dataset.map(separator)"
      ],
      "metadata": {
        "id": "ernmN6JeJ6Cx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukk0KU5TpH3a",
        "outputId": "37935e06-1516-41bb-cbe0-4039bf6d901f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Va ! endtoken'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Marche. endtoken'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken En route ! endtoken'], dtype=object)>)\n"
          ]
        }
      ],
      "source": [
        "for i in init_dataset.take(3):\n",
        "  print(i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCOUUsunpsix"
      },
      "source": [
        "### Vocab Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zDyJI27QpvNp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "9e94b197-eee6-400e-9e20-fd1c9bee65b1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4def4d973a4d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfrench_training_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# input x,y,z and output y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfrench_vectorize_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrench_training_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# adapt the vectorize_layer to the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/preprocessing/text_vectorization.py\u001b[0m in \u001b[0;36madapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    781\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;31m# Fast path for the case `self._structure` is not a nested structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m_from_compatible_tensor_list\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[0;31m# information.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m     \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m       raise ValueError(f\"Tensor's shape {self.shape} is not compatible \"\n\u001b[1;32m    552\u001b[0m                        f\"with supplied shape {shape}.\")\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;31m# `_tensor_shape` is declared and defined in the definition of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;31m# `EagerTensor`, in C.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "english_training_data=init_dataset.map(lambda x,y:x) # input x,y and output x\n",
        "english_vectorize_layer.adapt(english_training_data) # adapt the vectorize_layer to the training data\n",
        "\n",
        "french_training_data=init_dataset.map(lambda x,y:y) # input x,y,z and output y\n",
        "french_vectorize_layer.adapt(french_training_data) # adapt the vectorize_layer to the training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5DDzmf8L_fZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def vectorizer(inputs,output):\n",
        "  return {'input_1':english_vectorize_layer(inputs['input_1']),\n",
        "          'input_2':french_vectorize_layer(inputs['input_2'])},french_vectorize_layer(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDlEbt5XM4r5"
      },
      "outputs": [],
      "source": [
        "split_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW8hWSGHNAbZ"
      },
      "outputs": [],
      "source": [
        "dataset=split_dataset.map(vectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5R0qqzbNYFZ"
      },
      "outputs": [],
      "source": [
        "for i in init_dataset.take(3):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-KNkkdXNfBx"
      },
      "outputs": [],
      "source": [
        "for i in dataset.take(1):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQeOs1m8Nooa"
      },
      "outputs": [],
      "source": [
        "dataset=dataset.shuffle(2048).unbatch().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MPd8xmaN45p"
      },
      "outputs": [],
      "source": [
        "NUM_BATCHES=int(200000/BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3LvhBFsOCcy"
      },
      "outputs": [],
      "source": [
        "#@ Training and testing split\n",
        "train_dataset=dataset.take(int(0.9*NUM_BATCHES))\n",
        "val_dataset=dataset.skip(int(0.9*NUM_BATCHES))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXIaPrbnOWh5"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuLziKTkOZyh"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(model_size, SEQUENCE_LENGTH): # d_model\n",
        "  output = []\n",
        "  for pos in range(SEQUENCE_LENGTH):\n",
        "    PE = np.zeros((model_size)) # initilizing with zeros\n",
        "    for i in range(model_size):\n",
        "      if i % 2 == 0: # even positions, sin formula is used according to paper\n",
        "        PE[i] = np.sin(pos/(10000**(i/model_size)))\n",
        "      else: # odd positions, cos formula is used as mentioned in the paper\n",
        "        PE[i] = np.cos(pos/(10000**((i-1)/model_size)))\n",
        "    output.append(tf.expand_dims(PE, axis = 0))\n",
        "\n",
        "  out = tf.concat(output, axis=0)\n",
        "  out = tf.expand_dims(out, axis=0)\n",
        "  return tf.cast(out, dtype=tf.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqa_KTf3T5Is"
      },
      "source": [
        "### Input Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xflRdAcHT89U"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Lambda\n",
        "class Embeddings(Layer):\n",
        "  def __init__(self, sequence_length, vocab_size, embedding_dim):\n",
        "    super(Embeddings, self).__init__()\n",
        "    self.token_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = positional_encoding(self.embedding_dim, self.sequence_length) # PE adding here\n",
        "    return embedded_tokens + embedded_positions # final output for inputs\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "     return Lambda(lambda x: tf.math.not_equal(x, 0))(inputs) # masking function for checking if there are pad tokens(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noFlpky0YjJX"
      },
      "source": [
        "### Custome Attention Layer\n",
        "\n",
        "- Self attention layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DVoYgd-YrMN"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomSelfAttention(Layer):\n",
        "  def __init__(self, model_size):\n",
        "    super(CustomSelfAttention, self).__init__()\n",
        "    self.model_size = model_size\n",
        "\n",
        "  def call(self, query, key, value, masking):\n",
        "    #### Compute Scores ####\n",
        "    score = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    #### Scaling ####\n",
        "    score = score / tf.math.sqrt(tf.cast(self.model_size, dtype=tf.float32))\n",
        "\n",
        "    #### Masking ####\n",
        "    masking = tf.cast(masking, dtype=tf.float32)\n",
        "    score -= (1.0 - masking) * 1e10\n",
        "\n",
        "    #### Attention Weights ####\n",
        "    attention_weights = tf.nn.softmax(score, axis=-1) * masking\n",
        "\n",
        "    #### Weighted Sum ####\n",
        "    head_output = tf.matmul(attention_weights, value)\n",
        "\n",
        "    #### Output ####\n",
        "    return head_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-headed Attention\n",
        "- Multihead Attention allows model to focus on different part of input sequence simultaneously and combine these prespective into comprehensive representation.\n",
        "\n",
        "- For example: \"Harry saw a man with binoculars'. This sentence can have two meanings, they are either it can be harry saw a man using binoculars or it can be harry saw a man who has binoculars. These both can be correct. So transformer has to understand both these meanings which self-attention fails to recognize that's why multi-head attention is used."
      ],
      "metadata": {
        "id": "f1Vqrwn1Sdsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class CustomMultiHeadAttention(Layer):\n",
        "  def __init__(self, num_heads, key_dim):\n",
        "    super(CustomMultiHeadAttention, self).__init__()\n",
        "\n",
        "    self.num_heads = num_heads\n",
        "    self.dense_q = [Dense(key_dim//num_heads) for _ in range(num_heads)]\n",
        "    self.dense_k = [Dense(key_dim//num_heads) for _ in range(num_heads)]\n",
        "    self.dense_v = [Dense(key_dim//num_heads) for _ in range(num_heads)]\n",
        "    self.dense_o = Dense(key_dim)\n",
        "    self.attention = CustomSelfAttention(key_dim)\n",
        "\n",
        "  def call(self, query, key, value, attention_mask):\n",
        "    heads = []\n",
        "\n",
        "    for i in range(self.num_heads): # for each head\n",
        "      print(f'head-{i}', self.dense_q[i](query).shape)\n",
        "      head = self.self_attention(self.dense_q[i](query), self.dense_k[i](key),\n",
        "                                 self.dense_v[i](value), attention_mask)\n",
        "\n",
        "      heads.append(head)\n",
        "    heads = tf.concat(heads, axis=2) # concatenating all heads\n",
        "    heads = self.dense_o(heads) # passing all heads through a linear layer for the final output\n",
        "    return heads\n",
        "\n"
      ],
      "metadata": {
        "id": "XadG1w2LSioN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "PfIpdo0cZhm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TransformerEncoder(Layer):\n",
        "  def __init__(self, embedding_dims, dense_dims, num_heads):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    self.embedding_dims = embedding_dims\n",
        "    self.dense_dims = dense_dims\n",
        "    self.num_heads = num_heads\n",
        "    self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dims)\n",
        "\n",
        "    self.dense_proj = tf.keras.Sequential([\n",
        "        Dense(self.dense_dims, activation=\"relu\"),\n",
        "        Dense(self.embedding_dims),\n",
        "    ])\n",
        "    self.layernorm_1 = LayerNormalization()\n",
        "    self.layernorm_2 = LayerNormalization()\n",
        "    self.supports_masking = True\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    # print(mask)\n",
        "    if mask is not None:\n",
        "      mask = tf.cast(mask[:, tf.newaxis, :], dtype='int32')\n",
        "      # print(mask)\n",
        "      T = tf.shape(mask)[2]\n",
        "      padding_mask = tf.repeat(mask, T, axis=1)\n",
        "      # print(padding_mask)\n",
        "\n",
        "    attention_output = self.attention(query=inputs, value=inputs, key=inputs, attention_mask=padding_mask)\n",
        "\n",
        "    proj_input = self.layernorm_1(inputs + attention_output)\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "    return self.layernorm_2(proj_input + proj_output)\n"
      ],
      "metadata": {
        "id": "5PAqADAlZnyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "eNcbK0YVmEDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(Layer):\n",
        "  def __init__(self, embedding_dims, latent_dims, num_heads):\n",
        "    super(TransformerDecoder, self).__init__()\n",
        "    self.embedding_dims = embedding_dims\n",
        "    self.latent_dims = latent_dims\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_1 = MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embedding_dims\n",
        "    ) # self attention\n",
        "    self.attention_2 = MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embedding_dims\n",
        "    ) # cross-attention with encoder's outputs\n",
        "    self.dense_proj = tf.keras.Sequential(\n",
        "        [Dense(latent_dims, activation='relu'), Dense(embedding_dims)]\n",
        "    ) # feed forward layer\n",
        "    self.layernorm_1 = LayerNormalization() # layer norm for all three layers as in paper\n",
        "    self.layernorm_2 = LayerNormalization()\n",
        "    self.layernorm_3 = LayerNormalization()\n",
        "    self.supports_masking = True # this is special because of decoder\n",
        "\n",
        "  def call (self, inputs, encoder_outputs, enc_mask, mask=None):\n",
        "\n",
        "    combined_mask=None\n",
        "    cross_attn_mask = None\n",
        "\n",
        "    if mask is not None:\n",
        "      causal_mask = tf.linalg.band_part(\n",
        "          tf.ones([tf.shape(inputs)[0],\n",
        "                   tf.shape(inputs)[1],\n",
        "                   tf.shape(inputs)[1]], dtype=tf.int32), -1, 0)\n",
        "      # the role of causal mask is to prevent peeking into the future tokens for the decoder to predict better\n",
        "      # the band_part method makes it really easier to do this\n",
        "\n",
        "      mask = tf.cast(\n",
        "          mask[:, tf.newaxis, :], dtype='int32'\n",
        "      )\n",
        "      enc_mask = tf.cast(\n",
        "          enc_mask[:, tf.newaxis, :], dtype='int32'\n",
        "      )\n",
        "\n",
        "      T = tf.shape(mask)[2] # T is the number of queries from the decoder\n",
        "      padding_mask = tf.repeat(mask, T, axis=1)\n",
        "      cross_attn_mask = tf.repeat(enc_mask, T, axis=1)\n",
        "      combined_mask = tf.minimum(padding_mask, causal_mask) # the full mask for the masked mutli-head-connection\n",
        "      # print(f'Padding_mask: {padding_mask}')\n",
        "      # print(f'Causal_mask: {causal_mask}')\n",
        "      # print(f'Combined_mask: {combined_mask}')\n",
        "      # print(f'Cross_attention_mask: {cross_attn_mask}')\n",
        "\n",
        "    if combined_mask is None:\n",
        "            combined_mask = tf.ones([tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[1]], dtype=tf.int32)\n",
        "\n",
        "    if cross_attn_mask is None:\n",
        "            T = tf.shape(inputs)[1]  # Get the sequence length from inputs\n",
        "            cross_attn_mask = tf.ones([tf.shape(inputs)[0], T, T], dtype=tf.int32)\n",
        "\n",
        "\n",
        "    attention_output_1 = self.attention_1(\n",
        "        query=inputs, key=inputs, value=inputs,\n",
        "        attention_mask=combined_mask # the first layer which is the self attention for decoder\n",
        "    )\n",
        "\n",
        "    out_1 = self.layernorm_1(inputs + attention_output_1) # the first output + inputs added to be the input\n",
        "    # for the cross_attention layer\n",
        "\n",
        "    attention_output_2, scores = self.attention_2(\n",
        "        query=out_1, key=encoder_outputs, value=encoder_outputs,\n",
        "        attention_mask=cross_attn_mask,# the mask from cross attention just like encoder\n",
        "        return_attention_scores=True # returning score to visualize\n",
        "    )\n",
        "\n",
        "    out_2 = self.layernorm_2(out_1 + attention_output_2) # output 2 after adding and normalizing to be passed\n",
        "    # to feed forward layer for the final outputs\n",
        "\n",
        "    proj_output = self.dense_proj(out_2)\n",
        "\n",
        "    return self.layernorm_3(out_2 + proj_output), scores # the last norm layer"
      ],
      "metadata": {
        "id": "cKJioxNfmGoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full transformer Model"
      ],
      "metadata": {
        "id": "-ebLcx7XHYjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIMS = 512\n",
        "LATENT_DIMS = 2048\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 1\n",
        "NUM_EPOCHS = 10\n",
        "attention_scores = {}"
      ],
      "metadata": {
        "id": "pN6sDoGWHbvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = Input(shape=(None,), dtype='int64', name='input_1')\n",
        "embeddings = Embeddings(ENGLISH_SEQUENCE_LENGTH, VOCAB_SIZE, EMBEDDING_DIMS)\n",
        "x = embeddings(encoder_inputs)\n",
        "enc_mask = embeddings.compute_mask(encoder_inputs)\n",
        "\n",
        "\n",
        "for _ in range(NUM_LAYERS): # there can be N number of layers as mentioned by paper\n",
        "  x = TransformerEncoder(EMBEDDING_DIMS, LATENT_DIMS, NUM_HEADS)(x)\n",
        "encoder_outputs = x\n",
        "\n",
        "decoder_inputs = Input(shape=(None,), dtype='int64', name='input_2')\n",
        "x = Embeddings(FRENCH_SEQUENCE_LENGTH, VOCAB_SIZE, EMBEDDING_DIMS)(decoder_inputs)\n",
        "\n",
        "for i in range(NUM_LAYERS):\n",
        "  x, scores = TransformerDecoder(EMBEDDING_DIMS, LATENT_DIMS, NUM_HEADS)(x, encoder_outputs, enc_mask)\n",
        "  attention_scores[f'decoder_layer{i+1}_block2'] = scores\n",
        "\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "decoder_outputs = Dense(VOCAB_SIZE, activation='softmax')(x)\n",
        "\n",
        "attention_score_model = tf.keras.Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    attention_scores, name='attention_score_model'\n",
        ")\n",
        "\n",
        "\n",
        "transformer = tf.keras.Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    decoder_outputs, name='transformer'\n",
        ")\n",
        "\n",
        "transformer.summary()"
      ],
      "metadata": {
        "id": "KeI40mXvIMMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training:"
      ],
      "metadata": {
        "id": "HI6XdlRGs-BQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU (Bilingual Evaluation Understudy)\n",
        "\n",
        "BLEU is a metric for evaluating the quality of text generated by machine translation systems by comparing the generated text (candidate translation) with one or more reference translations. It provides a numerical score that reflects how closely the generated text matches the reference translations.\n",
        "\n",
        "---\n",
        "\n",
        "## How BLEU Works\n",
        "\n",
        "### 1. **n-gram Precision**\n",
        "- BLEU calculates how many n-grams (contiguous sequences of `n` words) in the candidate translation appear in the reference translations.\n",
        "- **n-grams** can range from unigrams (single words) to higher-order n-grams like bigrams (two words), trigrams (three words), etc.\n",
        "\n",
        "**Example:**\n",
        "Candidate: `the cat is on the mat`  \n",
        "Reference: `the cat sat on the mat`  \n",
        "- Unigrams: `the`, `cat`, `is`, `on`, `the`, `mat`\n",
        "- Bigrams: `the cat`, `cat is`, `is on`, `on the`, `the mat`\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Clipping**\n",
        "- To prevent overcounting, BLEU uses **clipping** for n-grams. The count of an n-gram in the candidate is clipped to the maximum count of that n-gram in the reference(s).\n",
        "\n",
        "**Example:**\n",
        "Candidate: `the the the the`  \n",
        "Reference: `the cat is on the mat`  \n",
        "- Without clipping: Unigram `the` count = 4.\n",
        "- With clipping: Unigram `the` count = 2 (since it appears twice in the reference).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Precision for Different n-grams**\n",
        "- BLEU computes precision for unigrams, bigrams, trigrams, and so on.\n",
        "\n",
        "**Formula for n-gram precision:**\n",
        "\\[\n",
        "P_n = \\frac{\\text{Number of clipped n-gram matches}}{\\text{Total number of candidate n-grams}}\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Geometric Mean of Precision Scores**\n",
        "- BLEU combines the precision scores of all n-grams using the **geometric mean**, giving equal weight to each precision:\n",
        "\\[\n",
        "P = \\left( \\prod_{n=1}^N P_n \\right)^{1/N}\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Brevity Penalty (BP)**\n",
        "- BLEU penalizes short translations that match n-grams but fail to capture the full meaning of the reference.\n",
        "\n",
        "**Brevity penalty formula:**\n",
        "\\[\n",
        "BP =\n",
        "\\begin{cases}\n",
        "1 & \\text{if } c > r, \\\\\n",
        "e^{1-r/c} & \\text{if } c \\leq r,\n",
        "\\end{cases}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( c \\) = length of the candidate translation.\n",
        "- \\( r \\) = length of the closest reference translation.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Final BLEU Score**\n",
        "- The BLEU score combines the geometric mean of n-gram precisions with the brevity penalty:\n",
        "\\[\n",
        "\\text{BLEU} = BP \\cdot P\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "## Strengths of BLEU\n",
        "1. **Language-Agnostic**: Works across languages as it relies on n-grams.\n",
        "2. **Multiple References**: Accommodates multiple reference translations to account for variability in phrasing.\n",
        "3. **Efficient**: Computationally inexpensive compared to human evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "## Limitations of BLEU\n",
        "1. **Surface-Level Matching**: Only matches n-grams and doesn't capture semantic meaning or grammatical correctness.\n",
        "2. **Insensitive to Context**: Ignores sentence structure and context.\n",
        "3. **Poor for Short Texts**: Fails to give meaningful scores for very short texts.\n",
        "4. **Overemphasis on Precision**: Does not explicitly account for recall, potentially penalizing candidates with good coverage but few exact matches.\n",
        "\n",
        "---\n",
        "\n",
        "## When to Use BLEU\n",
        "BLEU is widely used for:\n",
        "- **Machine Translation**: Evaluating the quality of translations.\n",
        "- **Text Generation**: Assessing tasks like text summarization or dialogue generation.\n",
        "- **Natural Language Processing Benchmarks**: Providing a standardized comparison metric.\n"
      ],
      "metadata": {
        "id": "IYMjZgqQtCA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BLEU(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name='bleu_score'):\n",
        "    super(BLEU, self).__init__()\n",
        "    self.bleu_score=0\n",
        "\n",
        "  def update_state(self, y_true, y_pred):\n",
        "    y_pred=tf.argmax(y_pred, -1)\n",
        "    self.bleu_score=0\n",
        "    for i, j in zip(y_pred, y_true):\n",
        "      tf.autograph.experimental.set_loop_options()\n",
        "\n",
        "      total_words=tf.math.count_nonzero(i)\n",
        "      total_matches=0\n",
        "      for word in i:\n",
        "        if word==0:\n",
        "          break\n",
        "        for q in range(len(j)):\n",
        "          if j[q]==0:\n",
        "            break\n",
        "          if word==j[q]:\n",
        "            total_matches+=1\n",
        "            j=tf.boolean_mask(j, [False if y==q else True for y in range(len(j))])\n",
        "            break\n",
        "\n",
        "       self.bleu_score+=total_matches/total_words\n",
        "\n",
        "  def result(set):\n",
        "    return self.bleu_score/BATCH_SIZE"
      ],
      "metadata": {
        "id": "i0kUNXw2tOP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Rate Schedular"
      ],
      "metadata": {
        "id": "YyA8MPzfyJOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers.schedules import LearningRateSchedule\n",
        "\n",
        "class Schedular(LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps):\n",
        "    super(Schedular, self).__init__()\n",
        "    self.d_model=tf.cast(d_model, tf.float64)\n",
        "    self.warmup_steps=tf.cast(warmup_steps, dtype=tf.float64)\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step=tf.cast(step, dtype=tf.float64)\n",
        "     return (self.d_model**(-0.5))*tf.math.minimum(step**(-0.5), step * (self.warmup_steps ** -1.5))"
      ],
      "metadata": {
        "id": "KCTJNfjXyMdn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPcYNFQ6dWQJY+7fJgc27eD"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}