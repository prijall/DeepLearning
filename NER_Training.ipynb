{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMt0M2642k1xWCby252iGSL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAMpvOPt8SYB",
        "outputId": "d92fbb13-2abb-40fa-fcc3-c43bb2ee42ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.7 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (0.9.11)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (1.5.2)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (4.66.6)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "#@ Importing Necessary dependencies and libraries:\n",
        "from nltk.tag import pos_tag\n",
        "!pip install sklearn-crfsuite\n",
        "from sklearn_crfsuite import CRF, metrics\n",
        "from sklearn.metrics import make_scorer, confusion_matrix\n",
        "from pprint import pprint\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Data Loading:\n",
        "def load_data_conll(file_path):\n",
        "  myoutput, tokens, tags=[], [], []\n",
        "  with open(file_path, 'r') as fh:\n",
        "    for line in fh:\n",
        "      line=line.strip() #to remove leading and trailing  white space characters\n",
        "      if '\\t' not in line:\n",
        "        #sentences ended\n",
        "        myoutput.append([tokens, tags])\n",
        "        tokens, tags=[], []\n",
        "      else:\n",
        "        token ,tag=line.split('\\t')\n",
        "        tokens.append(token)\n",
        "        tags.append(tag)\n",
        "    fh.close()\n",
        "    return myoutput"
      ],
      "metadata": {
        "id": "YN-MZG479LLg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting features of all the tokens in sentence.\n",
        "\n",
        "##### Features:\n",
        "- **Token Context:** a window of 2 tokens on either side of current token, and current token.\n",
        "\n",
        "- **POS Context:**  a window of 2 tokens on either side of current tag, and current tag.\n"
      ],
      "metadata": {
        "id": "kuoAGE14D8s3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence2features(sentence):\n",
        "  features=[]\n",
        "  sentence_tags=pos_tag(sentence)\n",
        "  for i in range(0, len(sentence)):\n",
        "    token=sentence[i]\n",
        "    tokenfeatures={}\n",
        "\n",
        "  #@ token features:\n",
        "   #token\n",
        "  tokenfeatures['token']=token\n",
        "\n",
        "  #for 2 prev tokens\n",
        "  if i==0:\n",
        "    tokenfeatures['prevtoken']=tokenfeatures['prevsecondtoken']='<S>'\n",
        "  elif i==1:\n",
        "    tokenfeatures['prevtoken']=sentence[0]\n",
        "    tokenfeatures['prevsecondtoken']='</S>'\n",
        "  else:\n",
        "    tokenfeatures['prevtoken']=sentence[i-1]\n",
        "    tokenfeatures['prevsecondtoken']=sentence[i-2]\n",
        "\n",
        "  #for 2 next token\n",
        "  if i==len(sentence)-2:\n",
        "    tokenfeatures['nexttoken']=sentence[i+1]\n",
        "    tokenfeatures['nextsecondtoken']='</S>'\n",
        "\n",
        "  elif i==len(sentence)-1:\n",
        "     tokenfeatures['nexttoken']='</S>'\n",
        "     tokenfeatures['nextsecondtoken']='</S>'\n",
        "\n",
        "  else:\n",
        "    tokenfeatures['nexttoken']=sentence[i+1]\n",
        "    tokenfeatures['nextsecondtoken']=sentence[i+2]\n",
        "\n",
        "  #@ POS feature:\n",
        "\n",
        "  #current tag\n",
        "  tokenfeatures['tag']=sentence_tags[i][1]\n",
        "\n",
        "  #prev tag\n",
        "  if i==0:\n",
        "    tokenfeatures['prevtag']=tokenfeatures['prevsecondtag']='</S>'\n",
        "  elif i==1:\n",
        "    tokenfeatures['prevtag']=sentence_tags[0][1]\n",
        "    tokenfeatures['prevsecondtag']='</S>'\n",
        "  else:\n",
        "     tokenfeatures['prevtag']=sentence_tags[i-1][1]\n",
        "     tokenfeatures['prevsecondtag']=sentence_tags[i-2][1]\n",
        "\n",
        "  #next tag\n",
        "  if i==len(sentence)-2:\n",
        "     tokenfeatures['nexttag']=sentence_tags[i+1][1]\n",
        "     tokenfeatures['nextsecondtag']='</S>'\n",
        "\n",
        "  elif i==len(sentence)-1:\n",
        "    tokenfeatures['nexttag']='</S>'\n",
        "    tokenfeatures['nextsecondtag']='</S>'\n",
        "\n",
        "  else:\n",
        "     tokenfeatures['nexttag']=sentence_tags[i+1][1]\n",
        "     tokenfeatures['nextsecondtag']=sentence_tags[i+2][i]\n",
        "\n",
        "  features.append(tokenfeatures)\n",
        "  return features"
      ],
      "metadata": {
        "id": "CeYXneB2E5wv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Extracting features:\n",
        "def get_features_conll(conll_data):\n",
        "  features=[]\n",
        "  labels=[]\n",
        "  for sentence in conll_data:\n",
        "    features.append(sentence2features(sentence[0]))\n",
        "    labels.append(sentence[1])\n",
        "  return features, labels"
      ],
      "metadata": {
        "id": "J-fjLGn2K9ni"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}