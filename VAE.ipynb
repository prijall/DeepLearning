{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLQWN6qlD/JGyOku4oWKPB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSg44qpOGr6y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How does VAE Works?\n",
        "1) The output of the encoder is two vectors:\n",
        "- Mean vector\n",
        "- Standard Deviation\n",
        "\n",
        "2) We sum the two vectors(mean and variation) which is mutliplied by small random number. The dimensions of the resultant vector will same to that of each vectors.\n",
        "\n",
        "3) The resultant vector is passed to the decoder to fetch the images.\n",
        "\n",
        "4) Loss value optimization is combination of:\n",
        "- KL Divergence loss: measures the deviation of distribution of mean vector and standard variance vector from 0 and 1 resp.\n",
        "-  MSE: optimization we use to reconstruct the image."
      ],
      "metadata": {
        "id": "5G9cd7tFJrG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "  def __init__(self, x_dim, h_dim1, h_dim2, latent_dim):\n",
        "    super(VAE, self).__init__()\n",
        "    #encoder section:\n",
        "    self.d1=nn.Linear(x_dim, h_dim1) #input -> hidden layer 1\n",
        "    self.d2=nn.Linear(h_dim1, h_dim2) #hidden layer 1 -> hidden layer 2\n",
        "\n",
        "    # mean and standard variation vectors:\n",
        "    self.d31=nn.Linear(h_dim2, latent_dim) # hidden layer 2 -> Mean\n",
        "    self.d32=nn.Linear(h_dim2, latent_dim) # hidden layer 2 -> log variance\n",
        "\n",
        "    #decoder section:\n",
        "    self.d4=nn.Linear(latent_dim, h_dim2) # latent -> hidden layer 2\n",
        "    self.d5=nn.Linear(h_dim2, h_dim1) #hidden layer 2 -> hidden layer 1\n",
        "    self.d6=nn.Linear(h_dim1, x_dim)\n",
        "\n",
        "  def encoder(self, x):\n",
        "    h=F.relu(self.d1(x))\n",
        "    h=F.relu(self.d2(h))\n",
        "    return self.d31(h), self.d32(h) # d31:vector for mean, d32:vector for log variance\n",
        "\n",
        "  def sampling(self, mean, log_var):\n",
        "    std=torch.exp(log_var * 0.5) #standard deviation\n",
        "    eps=torch.randn_like(std) # small random number\n",
        "    return eps.mul(std).add_(mean) # addition of mean ans std\n",
        "\n",
        "  def decoder(self, z):\n",
        "    h=F.relu(self.d4(x))\n",
        "    h=F.relu(self.d5(h))\n",
        "    return F.sigmoid(self.d6(h))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean, log_var= self.encoder(x.view(-1, 784))\n",
        "    z=self.sampling(mean, log_var)\n",
        "    return self.decoder(z), mean, log_var"
      ],
      "metadata": {
        "id": "7zzhpvWtG_W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_batch(data, model, optimizer, loss_function):\n",
        "  model.train()\n",
        "  data=data.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  reconstruct_batch, mean, log_var= model(data)\n",
        "  loss, mse, kld=loss_function(reconstruct_batch, data, mean, log_var)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss, mse, kld, log_var.mean(), mean.mean()\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_batch(data, model, loss_function):\n",
        "  model.eval()\n",
        "  data=data.to(device)\n",
        "  reconstruct, mean, log_var=model(data)\n",
        "  loss, mse, kld=loss_function(reconstruct, data, mean, log_var)\n",
        "  return loss, mse, kld, log_var.mean(), mean.mean()\n",
        "\n",
        "def loss_function(recon_x, x, mean, log_var):\n",
        "  RECON=F.mse(recon_x, x.view(-1, 784), reduction='sum')\n",
        "  KLD= -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
        "  return RECON + KLD, RECON, KLD\n"
      ],
      "metadata": {
        "id": "3bG9kqDY-OQU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}